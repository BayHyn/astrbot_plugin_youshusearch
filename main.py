import asyncio
import aiohttp
import random
import re
import base64
from typing import Dict, List, Optional
from urllib.parse import urljoin, quote

from astrbot.api.event import filter, AstrMessageEvent, MessageEventResult
from astrbot.api.star import Context, Star, register
import astrbot.api.message_components as Comp
from astrbot.api import logger

@register(
    "astrbot_plugin_youshusearch",  # æ’ä»¶ID
    "Foolllll",                    # ä½œè€…å
    "ä¼˜ä¹¦æœç´¢æ’ä»¶",                  # æ’ä»¶æ˜¾ç¤ºåç§°
    "1.2",                         # ç‰ˆæœ¬å·
    "https://github.com/Foolllll-J/astrbot_plugin_youshusearch", # æ’ä»¶ä»“åº“åœ°å€
)
class YoushuSearchPlugin(Star):
    def __init__(self, context: Context, config=None):
        super().__init__(context)
        if config is None:
            config = {}
        self.search_api_endpoint = "api/novel/search"
        self.base_api_url = config.get("base_url", "https://www.ypshuo.com/")
        self.COOKIE_STRING = config.get("cookie", "")

        logger.info(f"ä¼˜ä¹¦æœç´¢æ’ä»¶åˆå§‹åŒ–ï¼Œä½¿ç”¨çš„åŸºç¡€URL: {self.base_api_url}")
        if self.base_api_url == "https://www.ypshuo.com/":
            self.api = 1
            self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36",
            "Accept": "application/json, text/plain, */*",
            "Accept-Language": "zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
        }
        else:
            self.api = 2
            self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0", 
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
            "Cookie": self.COOKIE_STRING,
            "Referer": self.base_api_url
        }

        self.YS_PLATFORMS = {"ä»–ç«™", "æœ¬ç«™", "èµ·ç‚¹", "æ™‹æ±Ÿ", "ç•ªèŒ„", "åˆºçŒ¬çŒ«", "çºµæ¨ª", "é£å¢", "17K", "æœ‰æ¯’", "æ¯å£¤", "é“è¡€", "é€æµª", "æŒé˜…", "å¡”è¯»", "ç‹¬é˜…è¯»", "å°‘å¹´æ¢¦", "SF", "è±†ç“£", "çŸ¥ä¹", "å…¬ä¼—å·"}
        self.YS_CATEGORIES = {"ç„å¹»", "å¥‡å¹»", "æ­¦ä¾ ", "ä»™ä¾ ", "éƒ½å¸‚", "ç°å®", "å†›äº‹", "å†å²", "æ‚¬ç–‘", "æ¸¸æˆ", "ç«æŠ€", "ç§‘å¹»", "çµå¼‚", "äºŒæ¬¡å…ƒ", "åŒäºº", "å…¶ä»–", "ç©¿è¶Šæ—¶ç©º", "æ¶ç©ºå†å²", "æ€»è£è±ªé—¨", "éƒ½å¸‚è¨€æƒ…", "ä»™ä¾ å¥‡ç¼˜", "å¹»æƒ³è¨€æƒ…", "æ‚¬ç–‘æ¨ç†", "è€½ç¾çº¯çˆ±", "è¡ç”ŸåŒäºº", "è½»å°è¯´", "ç»¼åˆå…¶ä»–"}
        self.YS_STATUSES = {"è¿è½½ä¸­", "å·²å®Œç»“", "å·²å¤ªç›‘"}
        

    async def _perform_search(self, session: aiohttp.ClientSession, keyword: str, page: int = 1) -> Optional[tuple[List[Dict], int]]:
        """
        æ ¹æ®ç½‘ç«™ç‰ˆæœ¬æ‰§è¡Œæœç´¢ã€‚
        æˆåŠŸæ—¶è¿”å›ä¸€ä¸ªå…ƒç»„: (ç»“æœåˆ—è¡¨, æ€»é¡µæ•°)ã€‚
        å¤±è´¥æ—¶è¿”å› Noneã€‚
        """
        if self.api == 1:
            search_api_url = urljoin(self.base_api_url, self.search_api_endpoint)
            params = {"keyword": keyword, "page": str(page)}
            try:
                async with session.get(search_api_url, params=params, headers=self.headers, timeout=20) as response:
                    response.raise_for_status()
                    json_content = await response.json()
                    logger.info(f"æœç´¢ '{keyword}' (Page {page}) APIè°ƒç”¨æˆåŠŸã€‚")
                    if json_content.get("code") == "00" and "data" in json_content:
                        data = json_content["data"]
                        # APIè¿”å›çš„ç»“æœå­—å…¸å·²ç»å¾ˆä¸°å¯Œï¼Œç›´æ¥ä½¿ç”¨
                        results = data.get("data", []) 
                        total_pages = int(data.get("pageAll", 1))
                        return results, total_pages
                    else:
                        return None
            except Exception as e:
                logger.error(f"âŒ æ‰§è¡Œæ—§ç½‘å€APIæœç´¢æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
                return None

        elif self.api == 2:
            try:
                results_per_page = 20
                encoded_keyword = quote(keyword)
                search_url = urljoin(self.base_api_url, f"/search/all/{encoded_keyword}/{page}.html")
                logger.info(f"æ­£åœ¨è®¿é—®æœç´¢URL: {search_url}")

                async with session.get(search_url, headers=self.headers, timeout=20) as response:
                    response.raise_for_status()
                    html_content = await response.text()
                
                def clean_html(raw_html):
                    return re.sub(r'<[^>]+>', '', raw_html).strip()

                # åˆ¤æ–­é¡µé¢ç±»å‹
                if 'å…±æœ‰<b class="hot">' in html_content:
                    logger.info("æ£€æµ‹åˆ°æœç´¢ç»“æœåˆ—è¡¨é¡µï¼ŒæŒ‰åˆ—è¡¨è§£æã€‚")
                    total_results = 0
                    total_match = re.search(r'å…±æœ‰<b class="hot">\s*(\d+)\s*</b>æ¡ç»“æœ', html_content)
                    if total_match:
                        total_results = int(total_match.group(1))
                    
                    total_pages = (total_results + results_per_page - 1) // results_per_page if total_results > 0 else 1
                    
                    results = []
                    result_blocks = re.findall(r'<div class="c_row">.*?<div class="cb"></div>', html_content, re.DOTALL)
                    
                    for block in result_blocks:
                        book_info = {}
                        # æå–ä¹¦åå’ŒID
                        name_match = re.search(r'<span class="c_subject"><a href="/book/(\d+)">(.*?)</a></span>', block, re.DOTALL)
                        if name_match:
                            book_info['id'] = int(name_match.group(1))
                            book_info['novel_name'] = clean_html(name_match.group(2))
                        
                        # æå–ä½œè€…
                        author_match = re.search(r'<span class="c_label">ä½œè€…ï¼š</span><span class="c_value">(.*?)</span>', block, re.DOTALL)
                        if author_match:
                            book_info['author_name'] = clean_html(author_match.group(1))
                        
                        # æå–è¯„åˆ†
                        score_match = re.search(r'<span class="c_rr">([\d.]+)</span>', block)
                        if score_match:
                            book_info['score'] = score_match.group(1)
                        
                        # æå–è¯„åˆ†äººæ•°
                        scorer_match = re.search(r'<span class="stard">\((\d+)äººè¯„åˆ†\)</span>', block)
                        if scorer_match:
                            book_info['scorer'] = scorer_match.group(1)
                        
                        if 'id' in book_info and 'novel_name' in book_info:
                            results.append(book_info)
                    
                    logger.info(f"æˆåŠŸä»åˆ—è¡¨é¡µè§£æåˆ° {len(results)} æ¡ç»“æœï¼Œå…± {total_pages} é¡µã€‚")
                    return results, total_pages
                else:
                    logger.info("æœªæ‰¾åˆ°æœç´¢åˆ—è¡¨ï¼Œå°è¯•æŒ‰å•æœ¬ä¹¦ç±è¯¦æƒ…é¡µè§£æ...")
                    name_match = re.search(r'<title>(.*?)-.*?-ä¼˜ä¹¦ç½‘</title>', html_content)
                    id_match = re.search(r"uservote\.php\?id=(\d+)|rating\('\d+',\s*'(\d+)'\)|addbookcase\.php\?bid=(\d+)", html_content)

                    if name_match and id_match:
                        novel_id_str = next((gid for gid in id_match.groups() if gid is not None), None)
                        if novel_id_str:
                            novel_name = clean_html(name_match.group(1))
                            novel_id = int(novel_id_str)
                            logger.info(f"æœç´¢ç»“æœä¸ºç›´æ¥è·³è½¬ï¼Œè§£æåˆ°ä¹¦ç±: '{novel_name}' (ID: {novel_id})")
                            
                            results = [{'id': novel_id, 'novel_name': novel_name}]
                            total_pages = 1
                            return results, total_pages

                    logger.warning("é¡µé¢æ—¢ä¸æ˜¯æœç´¢åˆ—è¡¨ä¹Ÿä¸æ˜¯æœ‰æ•ˆçš„ä¹¦ç±è¯¦æƒ…é¡µï¼Œåˆ¤å®šä¸ºæ— ç»“æœã€‚")
                    return [], 0

            except Exception as e:
                logger.error(f"âŒ æ‰§è¡Œæ–°ç½‘å€æœç´¢æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
                return None
        
    async def _get_latest_novel_id(self, session: aiohttp.ClientSession) -> Optional[int]:
        """è·å–æœ€æ–°çš„å°è¯´ID"""
        if self.api == 1:
            url = "https://www.ypshuo.com/"
            try:
                async with session.get(url, headers=self.headers, timeout=10) as response:
                    response.raise_for_status()
                    html_content = await response.text()
                    
                    matches = re.findall(r'href="/novel/(\d+)\.html"', html_content)
                    
                    if matches:
                        latest_id = max([int(id) for id in matches])
                        logger.info(f"âœ… æˆåŠŸè·å–åˆ°æœ€æ–°çš„å°è¯´ID: {latest_id}")
                        return latest_id
                    else:
                        logger.warning("âŒ æœªèƒ½åœ¨ä¸»é¡µHTMLä¸­æ‰¾åˆ°æœ€æ–°çš„å°è¯´IDã€‚")
                        return None
            except aiohttp.ClientError as e:
                logger.error(f"âŒ è®¿é—®ä¸»é¡µå¤±è´¥: ç½‘ç»œæˆ–HTTPé”™è¯¯ - {e}")
                return None
            except Exception as e:
                logger.error(f"âŒ è·å–æœ€æ–°å°è¯´IDæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
                return None
        elif self.api == 2:
            url = "https://youshu.me/"
            try:
                async with session.get(url, headers=self.headers, timeout=10) as response:
                    response.raise_for_status()
                    html_content = await response.text()

                    new_book_section_match = re.search(
                        r'<div class="blocktitle">æ–°ä¹¦è‡ªåŠ©æ¨è.*?</div>\s*<div class="blockcontent">.*?</ul>',
                        html_content,
                        re.DOTALL
                    )

                    if not new_book_section_match:
                        logger.warning("âŒ æœªèƒ½åœ¨ä¸»é¡µHTMLä¸­æ‰¾åˆ°'æ–°ä¹¦è‡ªåŠ©æ¨è'åŒºå—ã€‚")
                        return None

                    new_book_block = new_book_section_match.group(0)
                    matches = re.findall(r'href="/book/(\d+)"', new_book_block)
                    
                    if matches:
                        latest_id = max([int(id) for id in matches])
                        logger.info(f"âœ… æˆåŠŸè·å–åˆ°æœ€æ–°çš„å°è¯´ID (youshu.me): {latest_id}")
                        return latest_id
                    else:
                        logger.warning("âŒ åœ¨'æ–°ä¹¦è‡ªåŠ©æ¨è'åŒºå—ä¸­æœªèƒ½æ‰¾åˆ°ä»»ä½•å°è¯´IDã€‚")
                        return None

            except aiohttp.ClientError as e:
                logger.error(f"âŒ è®¿é—®ä¸»é¡µå¤±è´¥ (youshu.me): ç½‘ç»œæˆ–HTTPé”™è¯¯ - {e}")
                return None
            except Exception as e:
                logger.error(f"âŒ è·å–æœ€æ–°å°è¯´IDæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ (youshu.me): {e}", exc_info=True)
                return None

    async def _get_novel_details_from_html(self, html_content: str, novel_id: str) -> Dict:
        """
        ä»HTMLå“åº”ä¸­æå–å°è¯´è¯¦ç»†ä¿¡æ¯çš„è¾…åŠ©å‡½æ•°ï¼Œä½¿ç”¨æ›´å¥å£®çš„DOMè§£æã€‚
        å·²ä¿®å¤å°é¢å›¾ç‰‡è·å–é€»è¾‘ã€‚
        """
        def clean_html_content(text):
            if not text:
                return 'æ— '
            text = re.sub(r'<[^>]+>', '', text)
            text = re.sub(r'\s+', ' ', text).strip()
            text = re.sub(r'\.{3,}å…¨æ–‡$', '...', text).strip()
            return text if text else 'æ— '
        
        novel_info = {}
        
        if self.api == 1:
            try:
                # 1. ä¼˜å…ˆä»OGPå…ƒæ ‡ç­¾ä¸­æå–å°é¢å›¾ç‰‡URL
                og_image_match = re.search(r'<meta[^>]*?name="og:image"[^>]*?content="(.*?)"', html_content)
                if og_image_match:
                    image_url = og_image_match.group(1)
                    # ç¡®ä¿URLæ˜¯å®Œæ•´çš„
                    if image_url.startswith('//'):
                        image_url = 'https:' + image_url
                    elif image_url.startswith('/'):
                        image_url = urljoin(self.base_api_url, image_url)
                    novel_info['image_url'] = image_url
                    logger.info(f"æå–åˆ°çš„å°é¢URL (ä»OGPæ ‡ç­¾): {image_url}")
                else:
                    # 2. å¦‚æœOGPæ ‡ç­¾ä¸å­˜åœ¨ï¼Œå†å›é€€åˆ°åŸæ¥çš„æ­£åˆ™åŒ¹é…æ–¹æ³•
                    image_match = re.search(r'<img src="(.*?)"[^>]*?class="book-img"', html_content)
                    if image_match:
                        image_url = image_match.group(1)
                        if image_url.startswith('/'):
                            image_url = urljoin(self.base_api_url, image_url)
                        novel_info['image_url'] = image_url
                        logger.info(f"æå–åˆ°çš„å°é¢URL (ä»<img>æ ‡ç­¾): {image_url}")
                    else:
                        novel_info['image_url'] = None
                        logger.warning("æœªèƒ½ä»é¡µé¢ä¸­æå–åˆ°å°é¢å›¾ç‰‡URLã€‚")


                # æå–ä¹¦å
                name_match = re.search(r'<h1 class="book-name".*?>(.*?)</h1>', html_content, re.DOTALL)
                novel_info['novel_name'] = name_match.group(1).strip() if name_match else 'æ— '
                
                # æå–ä½œè€…å
                author_match = re.search(r'ä½œè€…ï¼š<span class="text-red-500".*?>(.*?)</span>', html_content)
                novel_info['author_name'] = author_match.group(1).strip() if author_match else 'æ— '

                # æå–æ ‡ç­¾
                novel_info['tags'] = []
                tag_block_match = re.search(r'<div class="tag-list"[^>]*?>(.*?)</div>', html_content, re.DOTALL)
                if tag_block_match:
                    tag_html = tag_block_match.group(1)
                    # æŸ¥æ‰¾æ ‡ç­¾åŒºå—å†…çš„æ‰€æœ‰spanæ ‡ç­¾å†…å®¹
                    tags_list = re.findall(r'<span[^>]*?>(.*?)</span>', tag_html)
                    if tags_list:
                        # æ¸…ç†å¹¶å­˜å‚¨æ‰¾åˆ°çš„æ ‡ç­¾
                        novel_info['tags'] = [tag.strip() for tag in tags_list if tag.strip()]

                # æå–å­—æ•°
                word_count_match = re.search(r'å­—æ•°ï¼š(.*?)ä¸‡å­—', html_content)
                if word_count_match:
                    try:
                        word_str = word_count_match.group(1).strip().replace(',', '')
                        novel_info['word_number'] = float(word_str) * 10000
                    except (ValueError, TypeError):
                        novel_info['word_number'] = None
                else:
                    novel_info['word_number'] = None
                
                # æå–è¯„åˆ†å’Œè¯„åˆ†äººæ•°
                score_data_matches = re.findall(r'<div class="item"[^>]*?>\s*<p class="score"[^>]*?>\s*(.*?)\s*</p>\s*<p[^>]*?>(.*?)</p>\s*</div>', html_content, re.DOTALL)
                
                novel_info['score'] = 'æ— '
                novel_info['scorer'] = 'æ— '

                for value, label in score_data_matches:
                    if label.strip() == 'è¯„åˆ†':
                        novel_info['score'] = value.strip()
                    elif label.strip() == 'è¯„åˆ†äººæ•°':
                        novel_info['scorer'] = value.strip()
                
                # æå–çŠ¶æ€
                status_match = re.search(r'çŠ¶æ€ï¼š\s*(.*?)\s*<', html_content)
                novel_info['status'] = status_match.group(1).strip() if status_match else 'æ— '

                # æå–æ›´æ–°æ—¶é—´
                update_time_match = re.search(r'æ›´æ–°æ—¶é—´ï¼š\s*(.*?)\s*</div>', html_content)
                novel_info['update_time_str'] = update_time_match.group(1).strip() if update_time_match else 'æ— '
                
                reviews = []
                review_item_regex = re.compile(
                    r'<div class="author-info"[^>]*?>(.*?)</div>'  # ç¬¬1æ•è·ç»„: ä½œè€…å
                    r'.*?'                                         # åŒ¹é…ä¸­é—´æ‰€æœ‰å†…å®¹
                    r'aria-valuenow="([^"]+)"'                      # ç¬¬2æ•è·ç»„: è¯„åˆ†
                    r'.*?'                                         # åŒ¹é…ä¸­é—´æ‰€æœ‰å†…å®¹
                    r'<span class="content-inner-details"[^>]*?>(.*?)</span>', # ç¬¬3æ•è·ç»„: è¯„è®ºå†…å®¹
                    re.DOTALL
                )
                all_reviews = review_item_regex.findall(html_content)
                
                # å¾ªç¯ç°åœ¨ä¼šè§£åŒ…å‡º3ä¸ªå€¼
                for author, rating, content_block in all_reviews[:3]: 
                    content = re.sub(r'<[^>]+>', '', content_block)
                    content = re.sub(r'[\r\n\t]+', '', content).strip()
                    content = re.sub(r'\.{3,}å…¨æ–‡$', '...', content).strip()
                    
                    if content:
                        reviews.append({
                            'author': author.strip(), # æ·»åŠ ä½œè€…å
                            'content': content,
                            'rating': rating
                        })
                novel_info['reviews'] = reviews
                
                # æå–ç®€ä»‹
                synopsis_match = re.search(r'<div style="white-space:pre-wrap;"[^>]*?>(.*?)</div>', html_content, re.DOTALL)
                synopsis_content = synopsis_match.group(1).strip() if synopsis_match else 'æ— '
                novel_info['synopsis'] = synopsis_content
                
                # æå–é“¾æ¥
                link_match = re.search(r'<a href="(http.*?)".*?rel="nofollow".*?>', html_content)
                novel_info['link'] = link_match.group(1).strip() if link_match else 'æ— '

                return novel_info
                
            except Exception as e:
                logger.error(f"âŒ DOMè§£æå¤±è´¥ã€‚é”™è¯¯: {e}")
                return {}

        elif self.api == 2:
            try:
                # æå–ä¹¦å
                name_match = re.search(r'<title>(.*?)-.*?-ä¼˜ä¹¦ç½‘</title>', html_content)
                novel_info['novel_name'] = clean_html_content(name_match.group(1)) if name_match else 'æ— '

                # æå–ä½œè€…å
                author_match = re.search(r'ä½œè€…ï¼š<a.*?>(.*?)</a>', html_content)
                novel_info['author_name'] = clean_html_content(author_match.group(1)) if author_match else 'æ— '
                
                # æå–è¯„åˆ†å’Œè¯„åˆ†äººæ•°
                score_match = re.search(r'<span class="ratenum">(.*?)</span>', html_content)
                scorer_match = re.search(r'\((.*?)äººå·²è¯„\)', html_content)
                novel_info['score'] = clean_html_content(score_match.group(1)) if score_match else 'æ— '
                novel_info['scorer'] = clean_html_content(scorer_match.group(1)) if scorer_match else 'æ— '

                # æå–æ›´æ–°æ—¶é—´
                update_time_match = re.search(r'æœ€åæ›´æ–°ï¼š(.*?)</td>', html_content)
                novel_info['update_time_str'] = clean_html_content(update_time_match.group(1)) if update_time_match else 'æ— '

                # æå–ç®€ä»‹
                synopsis_match = re.search(r'<div class="tabvalue"[^>]*?>\s*<div[^>]*?>(.*?)</div>', html_content, re.DOTALL)
                novel_info['synopsis'] = clean_html_content(synopsis_match.group(1)) if synopsis_match else 'æ— '
                
                # æå–é˜…è¯»é“¾æ¥
                link_match = re.search(r'<a class="btnlink b_hot mbs" href="(.*?)"', html_content)
                novel_info['link'] = clean_html_content(link_match.group(1)) if link_match else 'æ— '

                # æå–å°é¢å›¾ç‰‡
                img_match = re.search(r'<a[^>]*?class="book-detail-img"[^>]*?><img src="(.*?)"', html_content)
                novel_info['image_url'] = urljoin(self.base_api_url, img_match.group(1).strip()) if img_match and img_match.group(1).strip() else None
                
                novel_info.update({'platform': 'æ— ', 'category': 'æ— ', 'status': 'æ— ', 'word_number': None})
                info_exp_match = re.search(r'<div class="author-item-exp">(.*?)</div>', html_content, re.DOTALL)
                if info_exp_match:
                    # 1. å…ˆç”¨ç‰¹æ®Šå­—ç¬¦æ›¿æ¢åˆ†éš”ç¬¦
                    raw_text = info_exp_match.group(1).replace('<i class="author-item-line"></i>', '|')
                    # 2. æ¸…ç†æ‰æ‰€æœ‰HTMLæ ‡ç­¾
                    clean_text = re.sub(r'<[^>]+>', '', raw_text)
                    # 3. æŒ‰ç‰¹æ®Šå­—ç¬¦åˆ†å‰²
                    info_parts = [part.strip() for part in clean_text.split('|') if part.strip()]
                    
                    # 4. éå†æå–å‡ºçš„æ¯ä¸ªéƒ¨åˆ†ï¼Œè¿›è¡Œæ™ºèƒ½åˆ†ç±»
                    for part in info_parts:
                        if part in self.YS_PLATFORMS:
                            novel_info['platform'] = part
                        elif part in self.YS_CATEGORIES:
                            novel_info['category'] = part
                        elif part in self.YS_STATUSES:
                            novel_info['status'] = part
                        elif 'å­—' in part:
                            word_match = re.search(r'(\d+)', part)
                            if word_match:
                                novel_info['word_number'] = float(word_match.group(1))

                novel_info['tags'] = []
                tag_section_match = re.search(r'<b>æ ‡ç­¾ï¼š</b>(.*?)</div>', html_content, re.DOTALL)
                if tag_section_match:
                    tag_block = tag_section_match.group(1)
                    tags = re.findall(r'<a[^>]*?>(.*?)</a>', tag_block)
                    if tags:
                        novel_info['tags'] = [clean_html_content(tag) for tag in tags]

                reviews = []
                review_blocks = re.findall(r'<div class="c_row cf">.*?<div class="c_tag">', html_content, re.DOTALL)

                for block in review_blocks[:5]:
                    author_match = re.search(r'<p>(.*?)</p></a>\s*<p><div class="user-level">', block, re.DOTALL)
                    rating_match = re.search(r'<span title="(\d+)\s*é¢—æ˜Ÿ"', block, re.DOTALL)
                    content_match = re.search(r'<div class="c_description">(.*?)</div>', block, re.DOTALL)

                    if author_match and rating_match and content_match:
                        author = clean_html_content(author_match.group(1))
                        logger.info(f"author: {author}")
                        rating = rating_match.group(1)
                        content = clean_html_content(content_match.group(1))
                        logger.info(f"content: {content}")
                        if content and content != 'æ— ':
                            reviews.append({
                                'author': author,
                                'content': content,
                                'rating': rating
                            })
                
                novel_info['reviews'] = reviews
                return novel_info
                
            except Exception as e:
                logger.error(f"âŒ DOMè§£æ (youshu.me) å¤±è´¥ã€‚é”™è¯¯: {e}")
                logger.info(f"å®Œæ•´HTMLå“åº”å†…å®¹: \n{html_content}")
                return {}
            
    async def _get_and_format_novel_details(self, event: AstrMessageEvent, session: aiohttp.ClientSession, novel_id: str):
        """
        æ ¹æ®å°è¯´IDè·å–ã€è§£æå¹¶æ ¼å¼åŒ–ä¹¦ç±è¯¦æƒ…ã€‚
        """
        if self.api == 1:
            novel_url = f"https://www.ypshuo.com/novel/{novel_id}.html"
        else:
            novel_url = f"https://youshu.me/book/{novel_id}"

        try:
            async with session.get(novel_url, headers=self.headers, timeout=10) as response:
                response.raise_for_status()
                html_content = await response.text()

            novel_info = await self._get_novel_details_from_html(html_content, str(novel_id))

            if not (novel_info and novel_info.get('novel_name', 'æ— ') != 'æ— '):
                raise ValueError(f"æ— æ³•ä»é¡µé¢ {novel_id} æå–æœ‰æ•ˆä¿¡æ¯ã€‚")

            if novel_info and novel_info.get('novel_name', 'æ— ') != 'æ— ':
                message_text = f"---ã€{novel_info.get('novel_name', 'æ— ')}ã€‘---\n"
                message_text += f"ä½œè€…: {novel_info.get('author_name', 'æ— ')}\n"

                if self.api == 2:
                    message_text += f"å¹³å°: {novel_info.get('platform', 'æœªçŸ¥')}\n"
                    message_text += f"åˆ†ç±»: {novel_info.get('category', 'æœªçŸ¥')}\n"
                
                tags = novel_info.get('tags')
                if tags:
                    message_text += f"æ ‡ç­¾: {' '.join(tags)}\n"

                word_number = novel_info.get('word_number')
                if word_number is not None and isinstance(word_number, (int, float)):
                    message_text += f"å­—æ•°: {word_number / 10000:.2f}ä¸‡å­—\n"
                else:
                    message_text += f"å­—æ•°: æ— \n"
                score = novel_info.get('score', 'æ— ')
                scorer = novel_info.get('scorer', 'æ— ')
                scorer_text = f"{scorer}äººè¯„åˆ†" if scorer and scorer != 'æ— ' else "æ— äººè¯„åˆ†"
                message_text += f"è¯„åˆ†: {score} ({scorer_text})\n"
                message_text += f"çŠ¶æ€: {novel_info.get('status', 'æ— ')}\n"
                message_text += f"æ›´æ–°: {novel_info.get('update_time_str', 'æ— ')}\n"
                synopsis = novel_info.get('synopsis', 'æ— ')
                message_text += f"ç®€ä»‹: {synopsis}\n"
                message_text += f"é“¾æ¥: {novel_info.get('link', novel_url)}\n"
                reviews = novel_info.get('reviews', [])
                if reviews:
                    message_text += "\n--- ğŸ“ æœ€æ–°ä¹¦è¯„ ---\n"
                    for review in reviews:
                        author = review.get('author', 'åŒ¿å')
                        rating = review.get('rating', 'æ— ')
                        content = review.get('content', 'æ— ')
                        message_text += f"{author} ({rating}åˆ†): {content}\n"
                
                chain = []
                if novel_info.get('image_url'):
                    image_url = novel_info['image_url']
                    try:
                        logger.info(f"æ­£åœ¨å°è¯•ä¸‹è½½å°é¢: {image_url}")
                        timeout = aiohttp.ClientTimeout(total=10)
                        async with session.get(image_url, timeout=timeout) as img_response:
                            img_response.raise_for_status()
                            image_bytes = await img_response.read()
                        
                        image_base64 = base64.b64encode(image_bytes).decode()
                        image_component = Comp.Image(file=f"base64://{image_base64}")
                        chain.append(image_component)

                    except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                        logger.warning(f"âŒ ä¸‹è½½å°é¢å›¾ç‰‡å¤±è´¥ (è¶…æ—¶æˆ–é“¾æ¥æ— æ•ˆ): {e}")
                        # ä¸‹è½½å¤±è´¥ï¼Œä»…åœ¨æ–‡æœ¬æ¶ˆæ¯å‰æ·»åŠ æç¤º
                        message_text = "ğŸ–¼ï¸ å°é¢åŠ è½½å¤±è´¥\n\n" + message_text
                
                chain.append(Comp.Plain(message_text))
                yield event.chain_result(chain)
                
            else:
                yield event.plain_result(f"ğŸ˜¢ æ— æ³•ä»é¡µé¢ {novel_id} æå–æœ‰æ•ˆä¿¡æ¯ã€‚")

        except aiohttp.ClientResponseError as e:
            logger.error(f"âŒ è®¿é—®è¯¦æƒ…é¡µ {novel_url} å¤±è´¥ï¼ŒHTTPçŠ¶æ€ç : {e.status}")
            raise e
        except Exception as e:
            logger.error(f"è§£æä¹¦ç±è¯¦æƒ…é¡µå¤±è´¥: {e}", exc_info=True)
            raise e

    @filter.command("ys") # å®šä¹‰æŒ‡ä»¤ /ys ä¹¦å [åºå· | -é¡µç ]
    async def youshu_search_command(self, event: AstrMessageEvent):
        """
        æœç´¢æœ‰ä¹¦ç½‘ä¸Šçš„ä¹¦ç±ä¿¡æ¯ã€‚
        ç”¨æ³•:
        - /ys <ä¹¦å>: æ˜¾ç¤ºç¬¬1é¡µæœç´¢ç»“æœåˆ—è¡¨ã€‚
        - /ys <ä¹¦å> <åºå·>: æ˜¾ç¤ºæŒ‡å®šåºå·çš„ä¹¦ç±è¯¦æƒ… (æ”¯æŒè·¨é¡µ)ã€‚
        - /ys <ä¹¦å> -<é¡µç >: æ˜¾ç¤ºæŒ‡å®šé¡µç çš„æœç´¢ç»“æœåˆ—è¡¨ã€‚
        """
        command_text = event.message_str.strip()
        command_parts = command_text.split()
        
        if not command_parts or command_parts[0].lower() != 'ys' or len(command_parts) < 2:
            yield event.plain_result("âŒ ç”¨æ³•: /ys <ä¹¦å> [åºå· | -é¡µç ]")
            return

        args = command_parts[1:]
        book_name, page_to_list, item_index = "", 1, None
        last_arg = args[-1] if args else ""
        if len(args) > 1 and last_arg.startswith('-') and last_arg[1:].isdigit():
            page_to_list = int(last_arg[1:])
            if page_to_list == 0: page_to_list = 1
            book_name = " ".join(args[:-1]).strip()
        elif len(args) > 1 and last_arg.isdigit():
            item_index = int(last_arg)
            if item_index == 0: item_index = None
            book_name = " ".join(args[:-1]).strip()
        else:
            book_name = " ".join(args).strip()
        if not book_name:
            yield event.plain_result("âŒ è¯·æä¾›æœ‰æ•ˆçš„ä¹¦åè¿›è¡Œæœç´¢ã€‚")
            return

        logger.info(f"ç”¨æˆ· {event.get_sender_id()} è§¦å‘ /ys, æœç´¢:'{book_name}', åºå·:{item_index}, åˆ—è¡¨é¡µ:{page_to_list}")
        
        try:
            async with aiohttp.ClientSession() as session:
                results_per_page = 20 if self.api == 2 else 15

                page_to_fetch = page_to_list
                if item_index is not None:
                    if item_index == 0:
                        yield event.plain_result("âŒ åºå·å¿…é¡»ä»1å¼€å§‹ã€‚")
                        return
                    page_to_fetch = (item_index - 1) // results_per_page + 1

                search_info = await self._perform_search(session, book_name, page=page_to_fetch)

                if search_info is None or (search_info[1] == 0 and page_to_fetch == 1 and not search_info[0]):
                    yield event.plain_result(f"ğŸ˜¢ æœªæ‰¾åˆ°å…³äºã€{book_name}ã€‘çš„ä»»ä½•ä¹¦ç±ä¿¡æ¯ã€‚")
                    return

                search_results, max_pages = search_info

                if page_to_fetch > max_pages and max_pages > 0:
                    yield event.plain_result(f"âŒ æ‚¨è¯·æ±‚çš„ç¬¬ {page_to_fetch} é¡µä¸å­˜åœ¨ï¼Œã€{book_name}ã€‘çš„æœç´¢ç»“æœæœ€å¤šåªæœ‰ {max_pages} é¡µã€‚")
                    return

                if item_index is None and len(search_results) == 1 and max_pages == 1:
                    logger.info(f"ğŸ” æœç´¢åˆ°å”¯ä¸€ç»“æœ for '{book_name}', ç›´æ¥æ˜¾ç¤ºè¯¦æƒ…ã€‚")
                    selected_book = search_results[0]
                    novel_id = selected_book.get('id')
                    if not novel_id:
                        yield event.plain_result("âŒ æ— æ³•è·å–è¯¥ä¹¦ç±çš„IDã€‚")
                        return
                    
                    # è°ƒç”¨è¯¦æƒ…å‡½æ•°å¹¶è¿”å›
                    async for result in self._get_and_format_novel_details(event, session, str(novel_id)):
                        yield result
                    return


                if item_index is None:
                    start_num = (page_to_fetch - 1) * results_per_page + 1
                    message_text = f"ä»¥ä¸‹æ˜¯ã€{book_name}ã€‘çš„ç¬¬ {page_to_fetch}/{max_pages} é¡µæœç´¢ç»“æœ:\n"
                    for i, book in enumerate(search_results):
                        num = start_num + i
                        name = book.get('novel_name', 'æœªçŸ¥ä¹¦ç±')
                        author = book.get('author_name', 'æœªçŸ¥ä½œè€…')
                        score = book.get('score', 'N/A')
                        scorer = book.get('scorer', '0')
                        message_text += f"{num}. {name}\n    ä½œè€…ï¼š{author} | è¯„åˆ†: {score} ({scorer}äºº)\n"
                    
                    message_text += f"\nğŸ’¡ è¯·ä½¿ç”¨ `/ys {book_name} <åºå·>` æŸ¥çœ‹è¯¦æƒ…"
                    if page_to_fetch < max_pages:
                        message_text += f"ï¼Œæˆ– `/ys {book_name} -{page_to_fetch + 1}` ç¿»é¡µã€‚"
                    yield event.plain_result(message_text)

                else:
                    index_on_page = (item_index - 1) % results_per_page
                    
                    if not (0 <= index_on_page < len(search_results)):
                        yield event.plain_result(f"âŒ åºå·ã€{item_index}ã€‘åœ¨ç¬¬ {page_to_fetch} é¡µä¸Šä¸å­˜åœ¨ã€‚")
                        return

                    selected_book = search_results[index_on_page]
                    novel_id = selected_book.get('id')
                    if not novel_id:
                        yield event.plain_result(f"âŒ æ— æ³•è·å–åºå·ä¸ºã€{item_index}ã€‘çš„ä¹¦ç±IDã€‚")
                        return
                    
                    logger.info(f"ğŸ” ç”¨æˆ·é€‰æ‹©åºå· {item_index}, è®¡ç®—é¡µç  {page_to_fetch}, ä¹¦ç±ID: {novel_id}")
                    
                    async for result in self._get_and_format_novel_details(event, session, str(novel_id)):
                        yield result

        except Exception as e:
            logger.error(f"æœç´¢ä¹¦ç± '{book_name}' å¤±è´¥: {e}", exc_info=True)
            yield event.plain_result(f"âŒ æœç´¢ä¹¦ç±æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}")

    @filter.command("éšæœºå°è¯´")
    async def youshu_random_command(self, event: AstrMessageEvent):
        """
        éšæœºè·å–ä¸€æœ¬ä¼˜ä¹¦ç½‘ä¸Šçš„å°è¯´ä¿¡æ¯ã€‚
        ç”¨æ³•: /éšæœºå°è¯´
        """
        max_retries = 10
        
        async with aiohttp.ClientSession() as session:
            try:
                latest_id = await self._get_latest_novel_id(session)
                if not latest_id:
                    yield event.plain_result("âŒ æŠ±æ­‰ï¼Œæœªèƒ½è·å–åˆ°æœ€æ–°çš„å°è¯´IDï¼Œæ— æ³•è¿›è¡Œéšæœºæœç´¢ã€‚")
                    return
            except Exception as e:
                logger.error(f"è·å–æœ€æ–°IDæ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
                yield event.plain_result("âŒ è·å–æœ€æ–°å°è¯´IDæ—¶å‡ºé”™ï¼Œè¯·ç¨åå†è¯•ã€‚")
                return

            for attempt in range(max_retries):
                random_id = random.randint(1, latest_id)
                logger.info(f"ç¬¬ {attempt + 1}/{max_retries} æ¬¡å°è¯•éšæœºID: {random_id}")
                
                try:
                    async for result in self._get_and_format_novel_details(event, session, str(random_id)):
                        yield result
                    
                    return

                except aiohttp.ClientResponseError as e:
                    if e.status == 404:
                        logger.warning(f"é¡µé¢ {random_id} ä¸å­˜åœ¨ (404)ï¼Œæ­£åœ¨é‡è¯•...")
                        continue # æ•è·åˆ°404é”™è¯¯ï¼Œé™é»˜é‡è¯•
                    else:
                        logger.error(f"è®¿é—®éšæœºé¡µé¢æ—¶å‘ç”ŸHTTPé”™è¯¯: {e.status}", exc_info=True)
                        yield event.plain_result(f"âŒ è®¿é—®éšæœºé¡µé¢æ—¶å‡ºé”™: HTTP {e.status}")
                        return
                except (ValueError, asyncio.TimeoutError) as e:
                    # æ•è·è§£æå¤±è´¥æˆ–è¶…æ—¶ï¼Œä¹Ÿé™é»˜é‡è¯•
                    logger.warning(f"å¤„ç†éšæœºID {random_id} å¤±è´¥: {e}ï¼Œæ­£åœ¨é‡è¯•...")
                    continue
                except Exception as e:
                    logger.error(f"å¤„ç†éšæœºID {random_id} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
                    yield event.plain_result(f"âŒ å¤„ç†éšæœºä¹¦ç±æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ã€‚")
                    return

        yield event.plain_result("ğŸ˜¢ æŠ±æ­‰ï¼Œå¤šæ¬¡å°è¯•åä»æœªæ‰¾åˆ°æœ‰æ•ˆçš„å°è¯´é¡µé¢ã€‚è¯·ç¨åå†è¯•ã€‚")

    async def terminate(self):
        """æ’ä»¶é”€æ¯æ—¶çš„æ¸…ç†å·¥ä½œ"""
        logger.info("ä¼˜ä¹¦æœç´¢æ’ä»¶å·²å¸è½½")